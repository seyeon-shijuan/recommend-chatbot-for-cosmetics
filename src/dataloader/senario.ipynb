{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 prompt:\n",
      "\t나 스킨 화장품 하나 만 추천 해줘\n",
      "전처리된 prepared sentences:\n",
      "\t['화장품 하나 추천 해줘\\n', '민감 성 피부 좋은 화장품 하나 추천 해주라\\n', '토너 몇개 만 추천 해줘\\n']\n",
      "코사인 유사도: 0.7740323183879741\n",
      "코사인 유사도: [0.77403232 0.30517734 0.2629022 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "    # 불용어 처리 (한국어)\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text)\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를', '것', '그', '에', '어', '하다']\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def cosine_similarity_between_preprocessed_korean_sentences(prompt, prepared_sentences):\n",
    "    # 전처리된 문장\n",
    "    preprocessed_sentence1 = preprocess_text(prompt)\n",
    "    preprocessed_prepared_sentences = [ preprocess_text(sentence) for sentence in prepared_sentences ]\n",
    "\n",
    "    # TF-IDF 벡터화 객체 생성\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # 두 문장을 합쳐서 벡터화\n",
    "    sentences = [preprocessed_sentence1, *preprocessed_prepared_sentences]\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    feature_vect_dense = tfidf_matrix.todense()\n",
    "    \n",
    "    vect1 = np.array(feature_vect_dense[0]).reshape(-1)\n",
    "    vect2 = np.array(feature_vect_dense[1]).reshape(-1)\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix)\n",
    "    cos_sim = cos_similarity(vect1, vect2)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"전처리된 prompt:\\n\\t{preprocessed_sentence1}\")\n",
    "    print_text = [ f\"{text}\\n\" for text in preprocessed_prepared_sentences]\n",
    "    print(f\"전처리된 prepared sentences:\\n\\t{print_text}\")\n",
    "    print(f\"코사인 유사도: {cos_sim}\")\n",
    "    print(f\"코사인 유사도: {cosine_sim[0][1:]}\")\n",
    "\n",
    "# 예시 문장 (한국어)\n",
    "prompt = \"나 스킨 화장품 하나만 추천해줘\"\n",
    "prepared_sentences = [\n",
    "    \"화장품 하나 추천해줘\",\n",
    "    \"민감성 피부에 좋은 화장품 하나 추천해주라\",\n",
    "    \"토너 몇개만 추천해줘\"\n",
    "]\n",
    "\n",
    "# 코사인 유사도 계산 (전처리 포함)\n",
    "cosine_similarity_between_preprocessed_korean_sentences(prompt, prepared_sentences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
