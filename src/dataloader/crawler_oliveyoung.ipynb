{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "# from tqdm import tqdm_notebook\n",
    "# from collections import OrderedDict\n",
    "import time\n",
    "# import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def create_directory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "# Selenium Driver에서 Beautiful Soup을 사용해 HTML 가져오기\n",
    "def get_soup(browser):\n",
    "    html = browser.page_source\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "# 페이지의 전체상품 상세링크 가져오기\n",
    "def get_product_links(soup):\n",
    "    # class가 \"prd_info\"인 모든 div 태그를 찾기\n",
    "    prd_info_divs = soup.find_all('div', class_='prd_info')\n",
    "\n",
    "    # 결과를 담을 리스트 초기화\n",
    "    href_list = []\n",
    "\n",
    "    # 각 div에서 class가 \"prd_thumb goodsList\"인 모든 a 태그의 href 속성 추출\n",
    "    for div in prd_info_divs:\n",
    "        prd_thumb_a = div.find('a', class_='prd_thumb goodsList')\n",
    "        if prd_thumb_a:\n",
    "            href = prd_thumb_a.get('href')\n",
    "            href_list.append(href)\n",
    "\n",
    "    return href_list\n",
    "\n",
    "\n",
    "# 페이지별 상품 리뷰 크롤링 함수2\n",
    "def review_crawling2(browser, category):\n",
    "    reviews = list()\n",
    "\n",
    "    # max 페이지\n",
    "    page_box = browser.find_elements(By.CSS_SELECTOR,'#gdasContentsArea > div > div.pageing > a')\n",
    "    max_page = len(page_box)\n",
    "\n",
    "    for page in range(1, max_page):\n",
    "        if page > 1:\n",
    "            try:\n",
    "                browser.find_elements(By.CSS_SELECTOR, '#gdasContentsArea > div > div.pageing > a')[page-2].click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            soup = get_soup(browser)\n",
    "            selectors = {\n",
    "                \"brand\": f'#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name',\n",
    "                \"nickname\": f'#gdasList > li:nth-child({i}) > div.info > div > p.info_user > a.id',\n",
    "                \"rate\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.score_area > span.review_point > span',\n",
    "                \"skin_type\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span',\n",
    "                \"select_1_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span',\n",
    "                \"select_2_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(2) > dd > span',\n",
    "                \"select_3_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(3) > dd > span',\n",
    "                \"txt\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.txt_inner'\n",
    "            }\n",
    "            # skin_type이랑 select_1_content랑 동일\n",
    "\n",
    "            contents = dict()\n",
    "\n",
    "            for name, selector in selectors.items():\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                # 텍스트 가져오기\n",
    "                if target_element:\n",
    "                    text_content = target_element.get_text(strip=True)\n",
    "                    contents[name] = text_content\n",
    "                    # print(\"텍스트:\", text_content)\n",
    "                else:\n",
    "                    print(f\"{name=}-해당하는 요소를 찾을 수 없습니다.\")\n",
    "                    break\n",
    "\n",
    "            # 제품명 특수문자 제거\n",
    "            brand_string = contents[\"brand\"]\n",
    "            brand = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', brand_string).strip().replace(\"  \", \" \")\n",
    "            contents[\"brand\"] = brand\n",
    "            contents[\"category\"] = category\n",
    "            try:\n",
    "                contents[\"rate\"] = contents[\"rate\"][-2]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            reviews.append(contents)\n",
    "\n",
    "    return browser, reviews\n",
    "\n",
    "\n",
    "# 페이지별 상품 리뷰 크롤링 함수\n",
    "def review_crawling(df, current_page):\n",
    "    for i in range(1, 11):  # 한 페이지 내 10개 리뷰 크롤링\n",
    "        try:\n",
    "            id = driver.find_element(By.CSS_SELECTOR,\n",
    "                                     f'#gdasList > li:nth-child({i}) > div.info > div > p.info_user > a.id').text\n",
    "            category = '스킨/토너'\n",
    "            brand_string = driver.find_element(By.CSS_SELECTOR,\n",
    "                                               '#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name').text\n",
    "            brand = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', brand_string).strip().replace(\"  \", \" \")\n",
    "            # if brand_string[0] == '[':\n",
    "            #     brand_match = re.search(r'\\]\\s*(.+?)\\s*\\(', brand_string)\n",
    "            #     brand = brand_match.group(1)\n",
    "            #     if brand_string[]\n",
    "            # else:\n",
    "            #     brand_match = re.search(r'(.+?)\\s*\\(', brand_string)\n",
    "            #     brand = brand_match.group(0)\n",
    "            # print(\"매칭되는 부분이 없습니다.\")\n",
    "            rate = driver.find_element(By.CSS_SELECTOR,\n",
    "                                       f'#gdasList > li:nth-child({i}) > div.review_cont > div.score_area > span.review_point > span').text\n",
    "            skin_type = driver.find_element(By.CSS_SELECTOR,\n",
    "                                            f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span').text\n",
    "            select_1_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span').text\n",
    "            select_2_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(2) > dd > span').text\n",
    "            select_3_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(3) > dd > span').text\n",
    "            txt = driver.find_element(By.CSS_SELECTOR,\n",
    "                                      f'#gdasList > li:nth-child({i}) > div.review_cont > div.txt_inner').text\n",
    "            df.loc[len(df)] = [id, category, brand, rate, skin_type, select_1_content, select_2_content,\n",
    "                               select_3_content, txt]\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {str(e)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_all_products_in_sub_category(name, path):\n",
    "    # 웹 드라이버 설정\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(path)\n",
    "\n",
    "    # 48개씩 보기 클릭\n",
    "    show48 = \"#Contents > div.cate_align_box > div.count_sort.tx_num > ul > li:nth-child(3) > a\"\n",
    "    driver.find_element(By.CSS_SELECTOR, show48).click()\n",
    "\n",
    "    # 소분류 전체 상품 링크\n",
    "    # max 페이지\n",
    "    page_box = driver.find_elements(By.CSS_SELECTOR, '#Container > div.pageing > a')\n",
    "    max_page = len(page_box)\n",
    "\n",
    "    sub_category_href_list = list()\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        soup = get_soup(driver)\n",
    "        href_list = get_product_links(soup)\n",
    "        sub_category_href_list.extend(href_list)\n",
    "\n",
    "        # 페이지 이동\n",
    "        if i == max_page:\n",
    "            break\n",
    "\n",
    "        a_index = (i - 1) % 10 + 1\n",
    "        a_element_xpath = f'//*[@id=\"Container\"]/div[2]/a[{a_index}]'\n",
    "        driver.find_element(By.XPATH, a_element_xpath).click()\n",
    "\n",
    "    # csv 저장\n",
    "    df = pd.DataFrame(sub_category_href_list, columns=['href'])\n",
    "    file_name = f\"data/{name}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "###########################################################################\n",
    "'''0. 폴더 생성'''\n",
    "category_directories = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\", \"./data/oil\"]\n",
    "for name in category_directories:\n",
    "    create_directory(name)\n",
    "\n",
    "''' 1. 카테고리 별 제품 링크 수집 (카테고리당 최대 480개 씩)\n",
    "(수집할 링크 목록 모을 때 사용)\n",
    "'''\n",
    "# category_dict = {\n",
    "#     \"스킨-토너\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010013&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0&trackingCd=Cat100000100010013_MID&trackingCd=Cat100000100010013_MID&t_page=%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EA%B4%80&t_click=%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EC%83%81%EC%84%B8_%EC%A4%91%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC&t_2nd_category_type=%EC%A4%91_%EC%8A%A4%ED%82%A8%2F%ED%86%A0%EB%84%88\",\n",
    "#     \"에센스-세럼-앰플\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010014&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"크림\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010015&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"로션\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010016&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"미스트-오일\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010010&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\"\n",
    "# }\n",
    "#\n",
    "# for name, path in category_dict.items():\n",
    "#     save_all_products_in_sub_category(name, path)\n",
    "\n",
    "'''2. 카테고리 제품 정보, 리뷰 데이터 수집'''\n",
    "\n",
    "# category_names = [\"스킨-토너\", \"에센스-세럼-앰플\", \"크림\", \"로션\", \"미스트-오일\"]\n",
    "# category_path = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\", \"./data/oil\"]\n",
    "category_names = [\"스킨-토너\", \"에센스-세럼-앰플\", \"크림\", \"로션\"]\n",
    "category_path = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\"]\n",
    "START_IDX = 100\n",
    "END_IDX = 190\n",
    "\n",
    "for cat_idx, to_collect in enumerate(category_names):\n",
    "    sub_category_href_df = pd.read_csv(f\"data/{to_collect}.csv\")\n",
    "    sub_category_href_df_start = sub_category_href_df.iloc[START_IDX:END_IDX, -1]\n",
    "    sub_category_href_list = list(sub_category_href_df_start)\n",
    "\n",
    "    # 웹 드라이버 설정\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # 데이터 셋 생성\n",
    "    review_cols = ['nickname', 'brand', 'category', 'rate', 'skin_type', 'select_1_content', 'select_2_content', 'select_3_content', 'txt']\n",
    "    df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "    df_ingredient = pd.DataFrame(columns=['name', 'ingredient'])\n",
    "\n",
    "    # 3. 소분류 제품 수집\n",
    "    for idx, link in enumerate(sub_category_href_list):\n",
    "        idx += START_IDX\n",
    "        driver.get(link)\n",
    "\n",
    "        # 리뷰 버튼 클릭\n",
    "        review_button = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/a')\n",
    "        review_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 리뷰 데이터 수집 (제품 1개)\n",
    "        driver, review_data = review_crawling2(driver, to_collect)\n",
    "        try:\n",
    "            tmp = pd.DataFrame(review_data)[review_cols]\n",
    "            df_review_cos1 = pd.concat([df_review_cos1, tmp], axis=0)\n",
    "            print(f\"{idx=}, {len(df_review_cos1)=}, {df_review_cos1.tail(1)}\")\n",
    "        except Exception as e:\n",
    "            print(\"idx: \", idx, e)\n",
    "            continue\n",
    "\n",
    "        if len(df_review_cos1) >= 1000:\n",
    "            filename = f\"{category_path[cat_idx]}/{to_collect}-{idx}.csv\"\n",
    "            df_review_cos1.to_csv(filename, index=False)\n",
    "            df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "            print(f\"created: {filename}\")\n",
    "\n",
    "    df_review_cos1.to_csv(f\"{category_path[cat_idx]}/{to_collect}-final.csv\", index=False)\n",
    "    df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "\n",
    "\n",
    "\n",
    "# # 전체 페이지 수 설정\n",
    "# total_pages = 15\n",
    "#\n",
    "# # 페이지 번호 초기화\n",
    "# page_number = 1\n",
    "#\n",
    "# while page_number <= 15:  # 페이지 끝 임의로 설정\n",
    "#     # 현재 페이지의 모든 화장품 이미지 버튼을 클릭\n",
    "#     for row in range(1, 7):\n",
    "#         time.sleep(2)\n",
    "#         for col in range(1, 5):\n",
    "#             image_button_xpath = f'//*[@id=\"Contents\"]/ul[{row + 1}]/li[{col}]/div/a'\n",
    "#             image_button = driver.find_element(By.XPATH, image_button_xpath)\n",
    "#             image_button.click()\n",
    "#             time.sleep(2)\n",
    "#\n",
    "#             # 리뷰 버튼 클릭\n",
    "#             review_button = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/a')\n",
    "#             review_button.click()\n",
    "#             time.sleep(1)\n",
    "#\n",
    "#             # 리뷰 데이터 수집\n",
    "#             review_data = review_crawling(df_review_cos1, page_number)\n",
    "#\n",
    "#             # 리뷰 및 정보 수집\n",
    "#             # for page_number in range(1, total_pages + 1):\n",
    "#             #     review_data=review_crawling(df_review_cos1, page_number)\n",
    "#\n",
    "#             # # 데이터프레임에 데이터 추가\n",
    "#             # if review_data is not None:\n",
    "#             #     df_review_cos1 = df_review_cos1.append(review_data, ignore_index=True)\n",
    "#             #     page_number += 1\n",
    "#\n",
    "#             # 구매 정보 클릭\n",
    "#             ingredient_button = driver.find_element(By.XPATH, '//*[@id=\"buyInfo\"]')\n",
    "#             ingredient_button.click()\n",
    "#             time.sleep(2)\n",
    "#\n",
    "#             # 이름, 성분 정보 수집\n",
    "#             name_string = driver.find_element(By.CSS_SELECTOR,\n",
    "#                                               '#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name').text\n",
    "#             name = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', name_string).strip().replace(\"  \", \" \")\n",
    "#\n",
    "#             ingredient = driver.find_element(By.CSS_SELECTOR, '#artcInfo > dl:nth-child(8) > dd').text\n",
    "#\n",
    "#             # df_ingredient에 데이터 추가\n",
    "#             df_to_add = pd.DataFrame({'name': [name], 'ingredient': [ingredient]})\n",
    "#             df_ingredient = pd.concat([df_ingredient, df_to_add], ignore_index=True)\n",
    "#\n",
    "#             # df_review_cos1에 데이터 추가\n",
    "#             df_review_cos1 = pd.concat([df_review_cos1, review_data], axis=0)\n",
    "#\n",
    "#             # 이미지 목록이 있는 페이지로 돌아가기\n",
    "#             driver.back()\n",
    "#\n",
    "#     # 리뷰 csv 파일 저장\n",
    "#     if df_review_cos1 is not None:\n",
    "#         df_review_cos1.to_csv(f'./scraping/data/review_data{page_number}.csv', index=False)\n",
    "#\n",
    "#     # 페이지 이동 후 로딩을 기다리기 위한 시간 지연\n",
    "#     time.sleep(2)\n",
    "#\n",
    "#     # 10 페이지가 되면 화살표 버튼을 클릭하여 다음 페이지로 이동\n",
    "#     if page_number % 10 == 0:\n",
    "#         # arrow_button_xpath = '//*[@id=\"Container\"]/div[2]/a[10]'\n",
    "#         # driver.find_element(By.XPATH, arrow_button_xpath).click()\n",
    "#         # time.sleep(2)\n",
    "#         break\n",
    "#     else:\n",
    "#         # 10 페이지가 아니면 a 태그 클릭\n",
    "#         a_index = (page_number - 1) % 10 + 1\n",
    "#         a_element_xpath = f'//*[@id=\"Container\"]/div[2]/a[{a_index}]'\n",
    "#         driver.find_element(By.XPATH, a_element_xpath).click()\n",
    "#\n",
    "#     # 페이지 번호 증가\n",
    "#     page_number += 1\n",
    "#\n",
    "# # 성분 csv 파일 저장\n",
    "# df_ingredient.to_csv(f'./scraping/data/ingredient_data{page_number}.csv', index=False)\n",
    "#\n",
    "# # 웹 드라이버 종료\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
